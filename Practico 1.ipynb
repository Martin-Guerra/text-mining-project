{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminacion de ruido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Convierto los archivos html en objetos soup para posterior analisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"framechapter1.html\") as fp:\n",
    "    soup1 = BeautifulSoup(fp)\n",
    "    \n",
    "with open(\"framechapter2.html\") as fp:\n",
    "    soup2 = BeautifulSoup(fp)\n",
    "    \n",
    "with open(\"framechapter3.html\") as fp:\n",
    "    soup3 = BeautifulSoup(fp)\n",
    "    \n",
    "with open(\"framechapter4.html\") as fp:\n",
    "    soup4 = BeautifulSoup(fp)\n",
    "    \n",
    "with open(\"framechapter5.html\") as fp:\n",
    "    soup5 = BeautifulSoup(fp)\n",
    "    \n",
    "with open(\"framechapter6.html\") as fp:\n",
    "    soup6 = BeautifulSoup(fp)\n",
    "    \n",
    "with open(\"framechapter7.html\") as fp:\n",
    "    soup7 = BeautifulSoup(fp)\n",
    "    \n",
    "with open(\"framechapter8.html\") as fp:\n",
    "    soup8 = BeautifulSoup(fp)\n",
    "    \n",
    "with open(\"framechapter9.html\") as fp:\n",
    "    soup9 = BeautifulSoup(fp)\n",
    "    \n",
    "with open(\"framechapter10.html\") as fp:\n",
    "    soup10 = BeautifulSoup(fp)\n",
    "    \n",
    "with open(\"framechapter11.html\") as fp:\n",
    "    soup11 = BeautifulSoup(fp)\n",
    "    \n",
    "with open(\"framechapter12.html\") as fp:\n",
    "    soup12 = BeautifulSoup(fp)\n",
    "    \n",
    "with open(\"framechapter13.html\") as fp:\n",
    "    soup13 = BeautifulSoup(fp)\n",
    "    \n",
    "with open(\"framechapter14.html\") as fp:\n",
    "    soup14 = BeautifulSoup(fp)\n",
    "    \n",
    "with open(\"framechapter15.html\") as fp:\n",
    "    soup15 = BeautifulSoup(fp)\n",
    "    \n",
    "with open(\"framechapter16.html\") as fp:\n",
    "    soup16 = BeautifulSoup(fp)\n",
    "    \n",
    "with open(\"framechapter17.html\") as fp:\n",
    "    soup17 = BeautifulSoup(fp)\n",
    "    \n",
    "with open(\"framechapter18.html\") as fp:\n",
    "    soup18 = BeautifulSoup(fp)\n",
    "    \n",
    "with open(\"framechapter19.html\") as fp:\n",
    "    soup19 = BeautifulSoup(fp)\n",
    "\n",
    "with open(\"framechapter20.html\") as fp:\n",
    "    soup20 = BeautifulSoup(fp)\n",
    "    \n",
    "with open(\"framechapter21.html\") as fp:\n",
    "    soup21 = BeautifulSoup(fp)\n",
    "    \n",
    "with open(\"framechapter22.html\") as fp:\n",
    "    soup22 = BeautifulSoup(fp)\n",
    "    \n",
    "with open(\"framechapter23.html\") as fp:\n",
    "    soup23 = BeautifulSoup(fp)\n",
    "    \n",
    "with open(\"framechapter24.html\") as fp:\n",
    "    soup24 = BeautifulSoup(fp)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Llevo todos los soup a la primera etiqueta p, que es donde esta la parte que queremos analizar; el texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup1 = soup1.p\n",
    "soup1 = soup1.get_text()\n",
    "\n",
    "soup2 = soup2.p\n",
    "soup2 = soup2.get_text()\n",
    "\n",
    "soup3 = soup3.p\n",
    "soup3 = soup3.get_text()\n",
    "\n",
    "soup4 = soup4.p\n",
    "soup4 = soup4.get_text()\n",
    "\n",
    "soup5 = soup5.p\n",
    "soup5 = soup5.get_text()\n",
    "\n",
    "soup6 = soup6.p\n",
    "soup6 = soup6.get_text()\n",
    "\n",
    "soup7 = soup7.p\n",
    "soup7 = soup7.get_text()\n",
    "\n",
    "soup8 = soup8.p\n",
    "soup8 = soup8.get_text()\n",
    "\n",
    "soup9 = soup9.p\n",
    "soup9 = soup9.get_text()\n",
    "\n",
    "soup10 = soup10.p\n",
    "soup10 = soup10.get_text()\n",
    "\n",
    "soup11 = soup11.p\n",
    "soup11 = soup11.get_text()\n",
    "\n",
    "soup12 = soup12.p\n",
    "soup12 = soup12.get_text()\n",
    "\n",
    "soup13 = soup13.p\n",
    "soup13 = soup13.get_text()\n",
    "\n",
    "soup14 = soup14.p\n",
    "soup14 = soup14.get_text()\n",
    "\n",
    "soup15 = soup15.p\n",
    "soup15 = soup15.get_text()\n",
    "\n",
    "soup16 = soup16.p\n",
    "soup16 = soup16.get_text()\n",
    "\n",
    "soup17 = soup17.p\n",
    "soup17 = soup17.get_text()\n",
    "\n",
    "soup18 = soup18.p\n",
    "soup18 = soup18.get_text()\n",
    "\n",
    "soup19 = soup19.p\n",
    "soup19 = soup19.get_text()\n",
    "\n",
    "soup20 = soup20.p\n",
    "soup20 = soup20.get_text()\n",
    "\n",
    "soup21 = soup21.p\n",
    "soup21 = soup21.get_text()\n",
    "\n",
    "soup22 = soup22.p\n",
    "soup22 = soup22.get_text()\n",
    "\n",
    "soup23 = soup23.p\n",
    "soup23 = soup23.get_text()\n",
    "\n",
    "soup24 = soup24.p\n",
    "soup24 = soup24.get_text()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Juntamos todos los soup conseguidos en uno solo para poder analizar el texto completo en una sola variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = soup1 + soup2 + soup3 + soup4 + soup5 + soup6 + soup7 + soup8 + soup9 + soup10 + soup11 + soup12 + soup13 + soup14 \n",
    "       + soup15 + soup16 + soup17 + soup18 + soup19 + soup20 + soup21 + soup22 + soup23 + soup24"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Eliminamos ahora los saltos de linea y finalizamos la etapa de eliminacion de ruido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = soup.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizacion"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Sacamos las contracciones de las palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions\n",
    "soup = contractions.fix(soup)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Importamos la libreria nltk, necesaria para generar los tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Separo las oraciones en palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = word_tokenize(soup)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Paso a minusculas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lowercase(texto):\n",
    "    new_words = []\n",
    "    for word in texto:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "texto = to_lowercase(texto)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Eliminacion de signos de puntuacion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_puntuacion(texto):\n",
    "    new_words = []\n",
    "    for word in texto:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "texto = remove_puntuacion(texto)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
